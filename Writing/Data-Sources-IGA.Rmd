---
title: Data and Methods for Analyzing Special Interest Influence in Rulemaking

author:
- name: Daniel Carpenter
  affiliation: Harvard University
#  footnote: 1737 Cambridge Street, Room 405, Cambridge, Massachusetts 02138
- name: Devin Judge-Lord
  affiliation: University of Wisconsin
#  footnote: 1059 Bascom Mall, 110 North Hall, Madison, WI 53706
- name: Brian Libgober
  affiliation: Yale University
#  footnote: 115 Prospect Street, Room 113, New Haven, CT 06520
- name: Steven Rashin
  affiliation: New York University
  footnote: "Corresponding Author: Steven.Rashin@NYU.edu, 19 West 4th St, 2nd Floor, New York, NY 10012"

abstract: The United States government creates astonishingly complete records relevant to policy creation in executive agencies. In this article, we describe the major kinds of data that have proven useful to scholars studying interest group behavior in bureaucratic politics, how to obtain them, and challenges that we as users have encountered in working with these data. We discuss databases such as regulations.gov, which contains comments on draft rules proposed by executive branch agencies, and new sources of data, such as ex-parte meeting logs, which describe the interest groups and individual lobbyists that bureaucrats are meeting face-to-face about proposed policies. One challenge is that some of these data are not machine-readable. We argue that a productive way forward is to invest in making all the datasets machine-readable and to create a consistent way to link them to each other as well as to outside databases.

keywords: Interest groups, rulemaking, lobbying, bureaucratic politics, data sources

output: 
   pdf_document:
      template: article-template.tex
      keep_tex: true
      latex_engine: pdflatex
      citation_package: natbib
      fig_caption: true
spacing: doublespacing
biblio-style: apsr2
bibliography: dataig.bib
endnotes: false # CHANGE THIS TO true BEFORE SUBMISSION 
frontpage: true

---

\noindent Submission ID: IGAD-D-19-00044

<!--
%Please remember that manuscripts should be limited to 3,000 - 5,000 words, including all tables, notes, and bibliographies.
If you plan to provide an online appendix, you may include that appendix with your initial submission, omitting it when you submit the final manuscript for production. That online appendix would not count toward the word limit. 
-->

# Introduction

If the U.S. federal government is unquestionably good at one thing, it is pushing out paper.
In theory, governmental records relevant to the creation of policy in executive agencies have long been available to researchers. Practically speaking, obtaining such data has been a costly and challenging undertaking. A decade or more ago, researchers were limited to acquiring data on just a few rules [e.g., @GoldenJPART1998] or using surveys [e.g., @FurlongJPART2004].
Since 1994, when the government first released the *Federal Register* online, scholars have had access to astonishingly detailed data on the notice-and-comment rulemaking process and, hence, regulatory policy, regulatory policymakers, and interest group advocacy. 

In this article, we describe the kinds of data that have proven useful to scholars studying interest group behavior in federal agencies, how to obtain them, and also challenges that we have found in working with these data. Three examples of data sources that fit this description are machine-readable records of all agency rules published since 1994, comments available on regulations.gov, and metadata about rules contained in the Unified Agenda. We also describe sources that have become more available in recent years, such as ex-parte meeting logs^[These logs record who participated in face-to-face or telephone meetings between interest groups and bureaucrats, often including individual names.] and individually-identified personnel records of nearly all federal employees since 1973 [@Singer-Vine2017]. 

These data sources do not exhaust the kinds of records relevant to researchers. They reflect what is available at present, but new data sources emerge constantly. Thus, we also highlight sources of data that we believe are already or may soon become available, perhaps after enterprising researchers submit the necessary Freedom Of Information Act (FOIA) requests. Examples include agency press releases [see, e.g., @LibgoberJOP,@Libgober2018] and the Foreign Agents Registration Act (FARA) reports [@ShepherdAPSR2019].^[We link to all the relevant data sources on our GitHub page: https://github.com/libgober/regdata/blob/master/README.md.]  To orient potential students of agency policymaking to available sources of data, we identify four units of analysis: (1) participants, (2) policymakers, (3) policy texts, and (4) metadata such as policy timing. We describe where to find data on each in roughly the order they appear in the process of developing a rule.

## Background: Rulemaking and the Administrative Procedures Act

For decades, the volume of legal requirements emerging from executive agencies has dwarfed the lawmaking activity of Congress, the Supreme Court, and the Presidency. Each year agencies publish four thousand or more regulations. In doing so, agencies are generally required to follow a process prescribed in Section 553 of the Administrative Procedures Act (APA). For each rule, agencies create a collection of documents that mark the stages of this policymaking process. Section 553 requires an agency to issue a Notice of Proposed Rulemaking (NPRM) in the *Federal Register* to notify potentially affected parties that the regulatory environment might change.
Following the NPRM, agencies solicit comments on these regulations through regulations.gov (for executive agencies) or the agency websites themselves (for independent agencies).
Agencies are required to consider the comments but are not required to alter the rules based on them. 
These documents are organized in a "docket" folder with a unique name and ID number. Once kept in a literal folder, these documents are now online. For example, the Centers for Medicare Medicaid Services rulemaking docket 0163 "Transparency in Coverage" can be found at regulations.gov/docket?D=CMS-2019-0163.
The fact that the APA requires agencies to assemble a comprehensive record of who sought influence over any part of the policy-making process, and that such records are relatively complete, is highly unusual in the American federal system and makes the area particularly exciting for interest group scholars.

# Who participates in the rulemaking process?

Notice and comment rulemaking creates a unique feature in American democracy; the right for anyone to participate in, perhaps even influence, most policymaking processes at federal agencies. Participation in rulemaking activities is highly skewed. A few rules, such as the Federal Communication Commission's rules on net neutrality, receive millions of comments. In contrast, half of proposed rules open for comment on regulations.gov receive no comments at all [@LibgoberJOP]. Even the median rule designated as "economically significant" -- rules projected to have an annual economic impact of over $100 million -- receives fewer than ten comments [@judgelord2019SPSA]. Participants include businesses, public interest groups, trade associations, unions, law firms, and academics [@CuellarALR2005,@YackeeJOP2006]. While businesses are the most consistent and apparently influential participants [@YackeeJOP2006,@LibgoberJOP], most comments are part of public pressure campaigns. Indeed, at least 39 million of the 48 million comments on proposed rules on regulations.gov were mobilized by just 100 organizations [@judgelord2019SPSA].

Scholarly interest in patterns of participation in the policymaking process relies on the notion that the participants in the process seek to influence the development of the rule.
For example, @YouJOP2017 finds that half of all spending on lobbying legislation occurs *after* a bill becomes law.
Participation in this process can begin even before a proposed rule is issued.
De @deFigureidoKimICC2004 show that meetings between agency officials and firms spike before an agency issues a policy order.
@LibgoberQJPS finds that firms that meet with federal regulators before a rule is issued may receive abnormally high stock market returns upon its release. Leveraging high-frequency trading data, Libgober shows that in the minutes and hours following the publication of proposed rules at the Federal Reserve, the firms that met with the Board during rule-development significantly outperformed matched market competitors who did not obtain such early access. These findings are consistent with the analysis of qualitative researchers that commenters who participate early in the rulemaking process can shape the content of the rule [@NaughtonJPAM2009].
In some cases, public participation can even initiate a rulemaking process via a petition for rulemaking.

Scholars are also interested in how participation in rulemaking may affect a rule's ultimate fate when sent to the White House Office of Management and Budget (OMB) or in judicial review. 
Interest groups lobby successfully in OMB review [@HaederYackee2015]  Interest groups also use the rulemaking record but build a case for litigation. 
Yet, there are few large-N studies linking lobbying in rulemaking to litigation or court decisions. 
@LibgoberRashin2018SPSA analyzed comments submitted to financial regulators and found that threats of litigation were rare but, when they did occur, were often followed by court decisions. 
@judgelord2016MPSA found no relationship between the number of comments and the likelihood that the Supreme Court upheld or struck down an agency rule and, like @SchuckElliott1990, found that the court is more likely to strike down policies made through the notice and comment process.

To analyze patterns of participation, scholars use data from a variety of sources.
Sources for data on participation are agency rulemaking dockets [@GoldenJPART1998,@YackeeJPART2006,@YoungBP2017,@BanBP2019], the *Federal Register* [@BallaAPSR1998,@WestPAR2004], and regulations.gov [@Balla2019, @GordonRashinJOP]. 
Though commenters are not generally required to disclose their names and affiliations, many do. The best current data sources for obtaining the names of the organizations that submit comments on federal regulations are regulations.gov and the websites of the independent agencies themselves.

<!--
Stuff removed for length
^[One guide to effective commenting notes that organized interests should tell the regulators exactly who they are and why the regulators should alter a rule based on their comment [@Stoll2011].] 
^[Individual states also have rulemaking procedures; every state now has a version of the Administrative Procedures Act in place [see, e.g., @JensenPRQ2010]. Scholarly work has analyzed commenting and lobbying behavior in Minnesota [@BoehmkeJPP2013] and California [@JewellJPART2006] among other states.]

-->

## Obtaining and working with data on comment participants

Scholars wishing to obtain data on comments and commenters from executive agencies generally use the website regulations.gov.
Actually obtaining the data requires overcoming a number of technical and bureaucratic hurdles.
First, bulk data from the website can only be accessed via an Application Programming Interface (API).^[An API is a set of procedures that allow a user to access data from a website in a structured way.
Some websites limit API usage by requiring users to get an API key; regulations.gov is one of those websites.]  This requires requesting an API key.^[By emailing regulations@erulemakinghelpdesk.com with "your name, email address, organization, and intended use of the API."  See https://regulationsgov.github.io/developers/ for more details.
Approval of API keys takes several days. We caution prospective users of the site that these keys can be deactivated without warning or acknowledgment of why they were deactivated.] Second, obtaining an accurate count of comments is not straightforward as agencies have different policies regarding duplicated comments^[The difference between the total number of reported comments and the number of comments on regulations.gov is often due to mass comment campaigns being grouped together.] and confidential business information.^[See e.g., @Lubbers2012 and https://www.regulations.gov/userNotice.] Third, obtaining the data is time-consuming.^[Regulations.gov is subject to a rate limit of 1,000 queries per hour which is especially problematic for scholars seeking to analyze the full text of comments, many of which are included only as attachments. Downloading an individual document requires calling the API twice, once for the docket information, which includes the attachment URL(s), and then a second time to download the linked file.]
As of October 4, 2019, Regulations.gov has 11,238,958 public submission documents representing over 70 million public comments and almost 1.5 million rules and other documents.    
<!--
For example, on an EPA rule on clean water, regulations.gov notes that 1,123,388 million comments were submitted, but only 20,594 are available on the website itself.
FDA is famous for this as well. Susan got them from FOIA.
-->  
Not all federal agencies post rulemaking documents to regulations.gov. 
Scholars studying participation in these agencies can often obtain data on participation in rulemaking from the agency websites.^[Some agencies have only some records on regulations.gov. For example, the Federal Energy Regulatory Commission has posted proposed rules on regulations.gov, but directed comments to be submitted by email and then posted them on its eLibrary website.
Unlike regulations.gov, most agency sites do not have an API and thus require scholars to use bespoke web scrapers.
We have examples of scrapers for several of these agencies on https://github.com/libgober/regdata.
While these websites are not rate-limited or gated with API keys, we recommend caution with the rate at which scholars solicit information from the servers to avoid security protocols that trigger a temporary ban.]

<!--
Based on experience, we have found that attachments cannot be downloaded at a rate of 1,000 per hour as some documents will fail to download or download as files with 0 bytes.
Instead, we recommend a delay of several seconds in between each iteration.
This approach, however, vastly increases the time required to scrape large numbers of comments.
The OCC, for example, allows confidential business information to be withheld while HHS and IRS do not.^[See e.g., https://www.federalregister.gov/documents/2019/09/04/2019-18992/agency-information-collection-activities-information-collection-renewal-comment-request-joint and https://www.federalregister.gov/documents/2019/05/03/2019-09121/request-for-information-regarding-state-relief-and-empowerment-waivers.] 
-->  

<!--
^[We recommend using rvest if coding in R and Scrapy in Python.
We have tutorials on how to scrape comments from regulations.gov and FERC.]
--> 

In addition to the challenges of obtaining the data, scholars also must choose how they want to preprocess the data before analyzing it.
For example, to answer questions about the types of organizations that participate in rulemaking, scholars first need to accurately identify these organizations so that they can be assigned the correct covariate data.
 This process, however, is not trivial as the same firm can be identified in numerous ways.
For example, Goldman Sachs submitted comments to the Securities and Exchange Commission (SEC) and Federal Reserve Board (FRB) as The Goldman Sachs Group, Inc, Goldman Sachs Co. LLC, Goldman Sachs Bank USA, Goldman, Sachs & Co., Goldman Sachs Execution & Clearing, LP, among other identifiers.
All of these entities need to be accurately matched to Goldman Sachs, including the Goldman Sachs Execution & Clearing comment which is on Goldman Sachs letterhead.^[See https://www.sec.gov/comments/s7-03-10/s70310-43.pdf.]  The problem can be more acute with associations such as the American Bar Association (ABA) sometimes commenting as itself and other times as individual sections of the ABA. One way scholars have been able to get around this problem is to use fuzzy matching.
Fuzzy matching is a process that compares two strings of text and gives a distance between them.
This procedure will assign a small distance between Goldman Sachs and Goldman, Sachs & Co. and a larger distance between Goldman Sachs and Merrill Lynch.
The distances and optimal thresholds depend on the algorithm used.
Fuzzy matching, however, is not a panacea as the same procedure will also show a small distance between JP Morgan and Morgan Stanley, which are two separate entities. It will also not reveal whether an ABA letter is from the American Banker's Association or the American Bar Association. 
Many organizations active on similar rulemaking issues share acronyms or have similar names. In expectation, however, noise generated from incorrect matching is likely to bias a scholar against finding any relationships within the data.

<!--
^[This process is particularly difficult for comments submitted by a coalition.
Suppose a scholar is interested in the relationship between assets and commenting behavior.
If a coalition of the Bank of America and Wells Fargo comment on a rule, should the scholar use their combined assets as their independent variable of interest, their average assets, or not assign covariate data to this coalition?  There is no obvious answer to this question] 
-->

# Who writes rules? 

Scholars recently began to study how policymakers' identities and networks affect the policymaking process. 
This is partially due to the release of new data sources such as the U.S. Office of Personnel Management's (OPM) data on government employees [e.g., @BoltonAMP2018],^[Note that the dataset @BoltonAMP2018 used is not public like the similar BuzzFeed data we discuss below.] Open Secrets' lobbying [e.g., @Baumgartner2009] and revolving door databases [e.g.,  @VidalAER2012,@BertrandAER2014], machine-readable lobbying disclosure act reports [e.g.,  @BoehmkeJPP2013,@YouJOP2017, @Dwidar2019SPSA], meeting logs [@LibgoberQJPS], and datasets of corporate board membership such as Boardex [e.g., @ShiveROF2016].
Scholars have also been able to study the identities of policymakers through the creative use of longstanding data sources such as the *Federal Register* [e.g., @CarriganPAR2019].


Work on the networks between policymakers and private interests has reshaped our understanding of the policymaking space.
By exploiting meeting logs, @LibgoberQJPS finds that comments and meetings with policymakers are associated with abnormal returns in the billions.
@CarriganPAR2019 find that the number of job functions of the bureaucrats who write the rules is associated with both decreases in the time an agency takes to promulgate a rule and increases in the probability that the rule will be overturned in court. @VidalAER2012 find evidence that, among lobbyists, connections to politicians are more valuable than issue expertise.
In fact, when a former Senate aide becomes a lobbyist, the Senator's retirement results in a substantial loss of income for that lobbyist.


<!-- Lobbying also increases the time it takes for agencies to discover corporate malfeasance [@YuJFQA2011].
 
-->

## Obtaining and Working with personnel and lobbying data

Scholars can obtain data on agency personnel from the BuzzFeed personnel data release. The BuzzFeed personnel data^[Available at https://archive.org/details/opm-federal-employment-data/page/n1.
(The complete dataset is over 30GB).] contains information on federal employees such as their salaries, job titles, and basic demographic data from 1973 through 2016.^[Updated personnel files are available through 2018 from the OPM itself here: https://www.fedscope.opm.gov/datadefn/index.asp]  
These are the most comprehensive personnel records publicly available, but they have significant limitations.
First, not all agencies and occupations are a part of this release.^[ The list includes at least 16 agencies and, within the covered agencies, law enforcement officers, nuclear engineers, and certain investigators (Buzzfeed, 2017).
See https://www.buzzfeednews.com/article/jsvine/sharing-hundreds-of-millions-of-federal-payroll-records]  
Second, some of the employees in these data do not have unique identification numbers. 
This means that common names such as "John Smith" match multiple employees; the Veterans Health Administration (VHA) employed 24 John Smiths in 2014, five with the same middle initial.


To obtain machine-readable data on the identities of domestic lobbyists, scholars use two databases from Open Secrets -- a nonprofit focused on tracking spending on politics in the US -- on administrative and Congressional lobbying.
Downloading the lobbying data is straightforward as it only requires an account to access the "bulk data" page.
The lobbying data comes from the required disclosures under the Lobbying Disclosure Act of 1995 (LDA).
Note that the reporting threshold varies by type of firm (in-house have a higher minimum reporting threshold than lobbying firms) and over time.^[See https://lobbyingdisclosure.house.gov/ldaguidance.pdf for details.] The lobbying data covers 1999 through 2018 and is broken up into seven tables that can easily be loaded into R or Python.^[Note that the raw data can be downloaded from the Secretary of the Senate's Office of Public Records here: https://www.senate.gov/legislative/Public_Disclosure/LDA_reports.htm]  We note, however, that the data is not complete as some lobbyists do not disclose required contacts and the data does not contain exact monetary amounts.^[See https://www.gao.gov/assets/700/698103.pdf]  We recommend Open Secret's database as it is easier to use than the raw Senate data.
Open Secrets also has a database on revolving door employees that shows the career paths or federal government workers that went to the private sector and are employed in some capacity where their work depends on interacting with the federal government.^[See https://www.opensecrets.org/revolving/methodology.php for details.]  Unlike their lobbying database, this one must be scraped as there is no option to download it using bulk data.^[Note that they promote academic work using their database, so they do not actively discourage scraping their database.]    

Lobbyists advocating for foreign clients are required to disclose these contacts under the Foreign Agent Registration Act.
Foreign agents often lobby bureaucratic agencies; for example, the state Israel retained law firm, Arnold & Porter, for advice on, among other issues, registering securities with the SEC.^[See https://efile.fara.gov/docs/1750-Supplemental-Statement-20110729-13.pdf]  The reports contain a multitude of data including the names of foreign entities, the firms representing them, the nature of the contact, and the specific officials contacted.
The data are astonishingly complete - the website contains all 6264 FARA registrants and their foreign contacts since July 3, 1942.
The FARA data can be accessed online through the Department of Justice.^[https://efile.fara.gov/ords/f?p=1381:1:13132679194789:::::]  Unlike other data discussed in this essay, the FARA data is *not* all in machine-readable form;^[The metadata - e.g., dates, registratrants, clients - are available in machine-readable form through the API or bulk data downloads page.] converting from PDF to a machine-readable format (e.g., .csv) is neither easy nor error-free.
As a result, scholars using the data only focus on a subset such as @You2019 who focuses on lobbying activities by the governments of Colombia, Panama, and South Korea on their free trade agreements from 2003 through 2012.
In the absence of a completely digitized archive, scholars can exploit the search functions to search by lobbying registrant (e.g., lobbying firm Squire Patton Boggs) or foreign principal (e.g., the government of Afghanistan).
Similar to the discussion of the diversity of names above, the same actor can participate under different names.
For example, the Embassy of the Islamic Republic of Afghanistan and the Embassy of Afghanistan are designated as separate entities under FARA.


Corporate executives, lawyers, and lobbyists often meet with agency officials to discuss rules.
Records of these meetings are often recorded by agency personnel.
Obtaining these data requires writing a web scraper for each agency that holds the meetings data since the data are held in different places on each website.
Since there are no uniform standards for reporting meeting data, the data differ substantially from agency to agency in both content and organization.
The Federal Reserve, for example, groups meetings by subject but not by rule,^[https://www.federalreserve.gov/regreform/communications-with-public.htm] the SEC groups meetings by rule, and the CFTC and FCC has all their meetings in one place.^[See https://www.cftc.gov/LawRegulation/DoddFrankAct/ExternalMeetings?page=6 and https://www.fcc.gov/proceedings-actions/ex-parte/archive-of-filings]   

# What do the rules say? 

All of the work discussed above relies on the notion that public participation and the rulewriters influence the content of rules; numerous studies have found that commenters influence rules [see, e.g.,@CuellarALR2005,@YackeeJOP2006,@NaughtonJPAM2009,@HaederAPSR2015,@HaederJPART2018] at agencies such as the Department of Labor [@YackeeJOP2006], the Department of Treasury [@CuellarALR2005], the Environmental Protection Agency [@WagnerALR2011], the Securities and Exchange Commission [@Rashin2019], and numerous other agencies. This finding is consistent across interviews [e.g., @FurlongJPART1998], qualitative analysis [e.g., @CuellarALR2005], and quantitative analysis [e.g., @YackeeJOP2006].
Firms and groups representing businesses' interests are influential in securing policy concessions from the rulemaking process [@YackeeJPART2006,@YackeeJOP2006,@Carpenter2013], particularly when they are unopposed.
Other scholars have found that agencies respond to citizens [@BallaPI2019] and medical professionals [@BallaAPSR1998,@GordonRashinJOP].
  
   
<!--
^[However, see @GoldenJPART1998 and @ElliottDLJ1992 for evidence that rules do not change substantially.] 
-->

## Obtaining and Working with Data on Rule Text 

One of the most basic functions of the bureaucracy is to issue regulations that set specific, enforceable requirements implementing a law.
The federal government publishes all rules in the *Federal Register*, accessible via FederalRegister.gov.
This database is comprehensive and has machine-readable records of all regulations published after 1994.^[For issues from March 1939 through 1993, see https://www.govinfo.gov/app/collection/fr/, although note that they might require optical character recognition to make them in a readable format.]  Scholars can access search for regulations by the *Federal Register* citation, e.g., 47 FR 3431, *Federal Register* number, or, more usefully, through their API; unlike other APIs mentioned in this essay, the *Federal Register* API does not have any rate limits.^[However we still recommend small delays to prevent flooding the server with requests.]

While the raw text of rules is relatively straightforward to obtain, there are several thorny theoretical and methodological issues scholars must overcome.^[We note that raw texts of rules are not available from regulations.gov or the independent agencies themselves.
As these rules are often in PDF form, converting them to .txt for analysis introduces errors into the text.]  First, the standard path from notice of proposed rulemaking (NPRM) to public comments to a final rule is not always straightforward.
Some rules are withdrawn before a final rule.
Other agencies issue interim final rules subject to comments. For studies that seek to compare the proposed and final rules, the most problematic rules are the ones where one proposed rule gets broken up into a few smaller final rules or the reverse, where small rules become bundled into one final rule. These rules pose challenges for inference as the processes that lead to amalgamation or separation are not well understood.
Second, scholars must decide whether, and to what extent, they should preprocess the rulemaking data before feeding the data to a text analysis algorithm.


<!--
^[See, for example, the prudential standards rule issued by the Federal Reserve to improve financial stability.] 
-->

# Rule metadata 

Rulemaking metadata, such as the time rules are released, allow scholars to answer questions about factors that affect the rulemaking environment.
Scholars have exploited this data to study questions about regulatory delay, agenda-setting, and the financial impact of rules.
Often lengthy delays between statutory promulgation and the issuance of regulations are the subject of recent empirical scholarly interest.
Scholars attribute various causes to regulatory delay including auditing [@ACSPSQ2013]; outside contacts [@BallaALR2011]; the political climate [@PotterJOP2017,@ThrowerPSQ2018]; personnel types [@CarriganPAR2019]; ossification [@YackeeGWLR2012]; deadlines [@LavertuJPART2012,@CarpenterAJPS2011,@BertelliPAR2019]; staffing levels [@BoltonJLEO2015].
These scholars relied on data from OIRA [@ACSPSQ2013,@BallaALR2011,@BoltonJLEO2015,@CarriganPAR2019], the Unified Agenda [@BertelliPAR2019,@PotterJOP2017,@LavertuJPART2012,@Potter2019], the *Federal Register* [@YackeeGWLR2012,@ThrowerPSQ2018], and FDA's drug approval and postmarket experience [@CarpenterAJPS2011].
Scholars have also exploited rule metadata to show that publicly-traded banks that submit comments account for $7 billion in excess returns [@Libgober2018].


<!--
Workman's data isn't public, so I'm going to ax this graf
Scholars have also exploited the Unified Agenda to study agenda setting.
Notably, @Workman2015 coded approximately 220,000 regulations by issue area; he exploits this novel dataset to advance a theory of the "dual dynamics" of the administrative state where agenda-setting over the rulemaking process is a function of information generated by bureaucrats and Congressional "tuning" of the information.
@WestJPART2012 assess the influences on agency discretion using a survey of bureaucratic "contact officers" and the Unified Agenda; they find that most agency decisions to issue rules "are made in the context of ongoing program implementation." 
-->

## Obtaining and working with the Unified Agenda and Press releases
    
Obtaining data from the Unified Agenda is relatively straightforward as all issues since 1995 are available online in machine-readable form.^[The Unified Agenda from 1983 through 1994 is available in the *Federal Register*.]. 
The Unified Agenda contains all the proposed regulations an agency plans to issue in the near future, making it an extremely useful data source for studying questions about agenda setting and timing (for example, see @Potter2019).
There are, however, significant limitations to these data.
First, agencies report their early-stage rulemaking to the Unified Agenda strategically [@NouSCLR2016].
Second, agencies do not list "failed" rules that did not become final rules in the Unified Agenda [@YackeeGWLR2012].
Third, @CoglianeseALR2016 note that the Unified Agenda misses much of the regulatory agency's work, including enforcement actions, adjudicatory actions, and decisions not to act. 

In addition to the disclosures mandated by law, agencies often issue press releases to notify the public of important agency actions.
Much like the meetings data discussed above, the policies regarding their storage and dissemination differ from agency to agency.
The Federal Reserve, for example, lists all press releases since 1996 on their website, while the SEC only has them from 2012.^[See https://www.federalreserve.gov/newsevents/pressreleases.htm and https://www.sec.gov/news/pressreleases]  When working with the press releases scholars often need data on the exact time documents were made available to the general public [see, e.g., @LibgoberJOP].
Obtaining the press release metadata requires exploiting Really Simple Syndication (RSS) feeds to extract the exact time a press release becomes public.

<!--
^[We have used Feedly, but any RSS reader should have accessible timing metadata.] 
-->  

# Discussion: Assembling Complete Databases  

The United States government releases troves of data on rulemaking but in forms that require substantial effort from scholars to be useful for research. Scholars working on bureaucratic politics face two primary data challenges going forward: assembling complete, machine-readable datasets of agency rulemaking activity and linking observations across datasets.
We see four fruitful data projects which could increase researcher efficiency by preventing duplicated efforts to download and clean data.
First, a complete database is needed to link commenting activity throughout the Federal government that can be downloaded in bulk, including comment text and metadata.
Second, a comprehensive database could link revolving door rulemakers from the OPM personnel records, LDA disclosure forms, and FARA data.
This database would be significantly improved by the addition of data from LinkedIn, but those data are not currently public.
Third, creating a database of all meeting activities throughout the federal government.
Finally, unique identifiers for each commenter would allow researchers to link commenting behavior to organizations across datasets.
These projects, once completed, will allow scholars to pursue novel research on political participation, influence, and public management. 

# Conflicts of Interest 
On behalf of all authors, the corresponding author states that there is no conflict of interest.
