---
title: Data and Methods for Analyzing Special Interest Influence in Rulemaking

author:
- name: Daniel Carpenter
  affiliation: Harvard University
#  footnote: 1737 Cambridge Street, Room 405, Cambridge, Massachusetts 02138
- name: Devin Judge-Lord
  affiliation: University of Wisconsin
#  footnote: 1059 Bascom Mall, 110 North Hall, Madison, WI 53706
- name: Brian Libgober
  affiliation: Yale University
#  footnote: 115 Prospect Street, Room 113, New Haven, CT 06520
- name: Steven Rashin
  affiliation: New York University
  footnote: "Corresponding Author: Steven.Rashin@NYU.edu, 19 West 4th St, 2nd Floor, New York, NY 10012"

abstract: The United States government creates astonishingly complete records relevant to policy creation in executive agencies. In this article, we describe the major kinds of data that have proven useful to scholars studying interest group behavior in bureaucratic politics, how to obtain them, and challenges that we as users have encountered in working with these data. We discuss established databases such as regulations.gov, which contains comments on draft rules proposed by executive branch agencies, and new sources of data, such as ex-parte meeting logs, which describe the interest groups and individual lobbyists that bureaucrats are meeting face-to-face about proposed policies. One challenge is that some of these data are not machine-readable. We argue that a productive way forward is to invest in making all the datasets machine-readable and to create a consistent way to link them to each other as well as to other databases.

keywords: Interest groups, rulemaking, lobbying, bureaucratic politics, data sources

output:
   pdf_document:
      template: article-template.tex
      keep_tex: true
      latex_engine: pdflatex
      citation_package: natbib
      fig_caption: true
spacing: doublespacing
biblio-style: agsm
bibliography: dataig.bib
endnotes: true # CHANGE THIS TO true BEFORE SUBMISSION
frontpage: true

---

\noindent Submission ID: IGAD-D-19-00044

<!--
%Please remember that manuscripts should be limited to 3,000 - 5,000 words, including all tables, notes, and bibliographies.
If you plan to provide an online appendix, you may include that appendix with your initial submission, omitting it when you submit the final manuscript for production. That online appendix would not count toward the word limit.

Citations
All submissions should follow the Harvard style of in-text parenthetical citations.

Notes
Explanatory notes should be kept to an absolute minimum. Final versions of articles must use endnotes, not footnotes.

Style
The original style goals are clarity and consistency. Authors may use either UK or US spelling, but they must do so consistently. For punctuation, please follow the standards for UK English. This entails:

- using single inverted commas (rather than double quotation marks) for quotes
- placing all inverted commas within other punctuation
- for the grammatical dash, using an en dash with a single space before and after the en dash (rather than an em dash without spaces)

R&R RESPONSE:
https://docs.google.com/document/d/1qxUJxtnuc4kl1-oEB7_ksZKmAxPtD9OVqogOmIFN8gs/edit?usp=sharing
-->

# Introduction

If the U.S. federal government is unquestionably good at one thing, it is pushing out paper.
In theory, governmental records relevant to the creation of regulatory policy via rulemaking have long been available to researchers. Particularly when compared with adjudication, enforcement, or other forms of individualized decision-making such as awarding grants or contracts, public rulemaking is supposed to be transparent and visible. The need for transparency in this quasi-legislative activity is all the higher given that roughly ten times as many rules are enacted as laws every year. Nevertheless, practically speaking, obtaining information on what is happening during rulemaking has been a costly and challenging undertaking. In the 1990s and early 2000s, leading research was limited to data on just a few rules [e.g., @GoldenJPART1998] or using surveys [e.g., @FurlongJPART2004].
Since 1994, when the government first released the *Federal Register* online, access to data on the notice-and-comment has become increasingly detailed, and now include data on draft policies, the activities of policymakers, and interest group advocacy [see @Yackee2019 for a recent review].

In this article, we describe data sources that have proven useful to scholars studying interest group lobbying of federal agencies, how to obtain them, and also challenges in working with these data. Some of the data sources we discuss are machine-readable records of all agency rules published in the *Federal Register*, comments posted on regulations.gov, metadata about rules contained in the Unified Agenda, and Office of Management and Budget (OMB) reports. We also describe sources that have become more available in recent years, such as ex-parte meeting logs and individually-identified personnel records of nearly all federal employees since 1973 [@Singer-Vine2017].

These data sources do not exhaust the kinds of records relevant to researchers. They reflect what is available at present, but new data sources emerge constantly. Thus, we also highlight sources of data where complete datasets may soon become available, perhaps after enterprising researchers submit the necessary Freedom Of Information Act (FOIA) requests. Examples include agency press releases [see, e.g., @LibgoberJOP;@Libgober2018] and the Foreign Agents Registration Act (FARA) reports [@ShepherdAPSR2019].^[We link to more relevant data sources on our GitHub page: https://github.com/libgober/regdata/blob/master/README.md.]  To orient potential students of agency policymaking to available sources of data, we identify four units of analysis: (1) participants, (2) policymakers, (3) policy texts, and (4) metadata such as policy timing. We describe where to find data on each in roughly the order that they appear in the process of developing a rule.

## Background: Rulemaking and the Administrative Procedures Act

For decades, the volume of legal requirements emerging from executive agencies has dwarfed the lawmaking activity of Congress, the Supreme Court, and the Presidency.
Each year, agencies publish three thousand or more regulations. In doing so, agencies typically follow a process prescribed by the Administrative Procedures Act (APA), 5 U.S.C. ยง 553(c).
For each rule, agencies create a collection of documents at each stage of their policymaking process.
Section 553 requires agencies to issue a Notice of Proposed Rulemaking (NPRM) in the *Federal Register* to notify potentially affected parties that the regulatory environment or program administration may change.
Following the NPRM, agencies post these proposed rules and comments from interested stakeholders and individuals on their websites or regulations.gov.
Agencies are required to consider public comments but are not required to alter the rules based on them.
These documents are organized in a 'docket' folder with a unique name and ID number. Once kept in literal folders in each agency's 'docket room', where visitors could make photocopies, these documents are now online.
For example, the Centers for Medicare & Medicaid Services rulemaking docket on health plans disclosing costs is regulations.gov/docket?D=CMS-2019-0163.
The fact that the APA requires agencies to assemble a comprehensive record of who sought to influence the policymaking process -- and that such records are relatively complete -- is unusual for federal policymaking. This makes agency policymaking particularly exciting for interest group scholars.

# Who participates in the rulemaking process?

Public attention in rulemaking is remarkably skewed. A few rules, such as the Federal Communication Commission's rules on net neutrality, receive millions of comments. In contrast, half of the proposed rules open for comment on regulations.gov received no comments at all [@LibgoberJOP]. Such dramatic variation in public participation makes it difficult to think about the representative rule or rulemaking participant. Important participants include businesses, public interest groups, trade associations, unions, law firms, and academics [@CuellarALR2005;@YackeeJOP2006]. While businesses are the most consistent and influential participants in most rulemakings [@YackeeJOP2006;@LibgoberJOP], most comments are part of public pressure campaigns targeting a relatively small number of rules -- at least 39 million of the 48 million comments on proposed rules on regulations.gov were mobilized by just 100 advocacy organizations such as the Sierra Club [@judgelord2019SPSA].

<!--
Even the median rule designated as 'economically significant' -- rules projected to have an annual economic impact of over $100 million -- receives fewer than ten comments [@judgelord2019SPSA].
-->

Because interest groups seek to influence rules, scholars are interested in patterns of participation in rulemaking.
For example, @YouJOP2017 finds that half of all spending on lobbying legislation occurs *after* a bill becomes law.
Participation in this process can begin even before a proposed rule is issued.
@deFigureidoKimICC2004 show that meetings between agency officials and firms spike before an agency issues a policy.
@LibgoberQJPS finds that firms that meet with federal regulators before a rule is issued may receive abnormally high stock market returns upon its release. Leveraging high-frequency trading data, Libgober shows that in the minutes and hours following the publication of proposed rules at the Federal Reserve, the firms that met with the Board during rule-development significantly outperformed matched market competitors that did not obtain such early access. These findings are consistent with the analysis of qualitative researchers that commenters who participate early in the rulemaking process can shape the content of the rule [@NaughtonJPAM2009].

<!--
Interest groups and firms may even initiate a rulemaking process through a petition for rulemaking under 5 U.S.C. ยง 553(e).
-->

Scholars are also interested in how participation in rulemaking may affect a rule's ultimate fate when it is sent to the White House Office of Management and Budget (OMB) or in judicial review.
Interest groups lobby successfully in OMB review [@HaederAPSR2015].  Interest groups also use the rulemaking record to build a case for litigation.
Yet, there are few large-N studies linking lobbying in rulemaking to litigation or court decisions.
@LibgoberRashin2018SPSA analyzed comments submitted to financial regulators and found that threats of litigation were rare, and even comments that seemed to "threaten" litigation rarely culminated in a judicial decision. That said, the overwhelming majority of court decisions involving challenges to rules were preceded by comments that threatened litigation.
@judgelord2016MPSA found no relationship between the number of comments and the likelihood that the Supreme Court upheld or struck down an agency rule.

<!--
and, like @SchuckElliott1990, found that the court is more likely to strike down agency policies made through the notice and comment process.
-->

To analyze patterns of participation, scholars use data from a variety of sources.
Sources for data on participation are agency rulemaking dockets [@GoldenJPART1998;@YackeeJPART2006;@YoungBP2017;@BanBP2019], the *Federal Register* [@BallaAPSR1998;@WestPAR2004], and regulations.gov [@BallaPI2019;@GordonRashinJOP].
Though commenters are not generally required to disclose their names and affiliations, many do. The best current data sources for obtaining the names of the organizations that submit comments on federal regulations are regulations.gov and the websites of the independent agencies themselves.

<!--
Stuff removed for length
^[One guide to effective commenting notes that organized interests should tell the regulators exactly who they are and why the regulators should alter a rule based on their comment [@Stoll2011].]
^[Individual states also have rulemaking procedures; every state now has a version of the Administrative Procedures Act in place [see, e.g., @JensenPRQ2010]. Scholarly work has analyzed commenting and lobbying behavior in Minnesota [@BoehmkeJPP2013] and California [@JewellJPART2006] among other states.]

-->

## Obtaining and working with data on comment participants

Scholars wishing to obtain data on comments and commenters from executive agencies generally use the website regulations.gov.
Actually obtaining the data requires overcoming a number of technical and bureaucratic hurdles.
First, bulk data from the website can only be accessed via an Application Programming Interface (API).^[An API is a set of procedures that allow a user to access data from a website in a structured way.
Some websites, like regulations.gov, limit API usage by requiring users to get an API key.]
<!---
^[By emailing regulations@erulemakinghelpdesk.com with 'your name, email address, organization, and intended use of the API'.  See https://regulationsgov.github.io/developers/ for more details.
Approval of API keys takes several days. We caution prospective users of the site that these keys can be deactivated without warning or acknowledgment of why they were deactivated.]
-->
Second, obtaining an accurate count of comments is not straightforward as agencies have different policies regarding duplicated comments^[The difference between the total number of reported comments and the number of comments on regulations.gov is often because mass comment campaigns are grouped together. Occasionally, comments on regulations.gov are duplicated. To resolve a discrepancy, we recommend searching the agency's website or contacting the agency directly.] and confidential business information [@Lubbers2012]. Third, obtaining the data is time-consuming. Regulations.gov is subject to a rate limit of 1,000 queries per hour,  problematic for scholars seeking to analyze the full text of comments, many of which are as attachments. Downloading an individual document requires calling the API twice, once for the docket information, which includes the attachment URL(s), and then a second time to download the linked file.
As of October 4, 2019, Regulations.gov has 11,238,958 public submission documents representing over 70 million public comments and almost 1.5 million rules and other documents.  Not all federal agencies post rulemaking documents to regulations.gov.
Scholars studying participation in these agencies can often obtain data on participation in rulemaking from the agency websites.^[For example, the Federal Energy Regulatory Commission has posted all comments they receive on their eLibrary website, but not all of these appear on regulations.gov.
Unlike regulations.gov, most agency sites do not have an API and thus require bespoke web scrapers.
We have examples of scrapers for regulations.gov and several of these agencies on https://github.com/libgober/regdata.]

<!--
While these websites are not rate-limited or gated with API keys, we recommend caution with the rate at which scholars solicit information from the servers to avoid security protocols that trigger a temporary ban.]  

For example, on an EPA rule on clean water, regulations.gov notes that 1,123,388 million comments were submitted, but only 20,594 are available on the website itself. FDA is famous for this as well. Susan got them from FOIA.

Based on experience, we have found that attachments cannot be downloaded at a rate of 1,000 per hour as some documents will fail to download or download as files with 0 bytes.
Instead, we recommend a delay of several seconds in between each iteration.
This approach, however, vastly increases the time required to scrape large numbers of comments.
The OCC, for example, allows confidential business information to be withheld while HHS and IRS do not.^[See e.g., https://www.federalregister.gov/documents/2019/09/04/2019-18992/agency-information-collection-activities-information-collection-renewal-comment-request-joint and https://www.federalregister.gov/documents/2019/05/03/2019-09121/request-for-information-regarding-state-relief-and-empowerment-waivers.]

^[We recommend using rvest if coding in R and Scrapy in Python.
We have tutorials on how to scrape comments from regulations.gov and FERC.]
-->

\indent After obtaining comment data, scholars must choose how they want to preprocess the names of the organizations that comment.  This step is vital as organizations such as Goldman Sachs and the American Bar Association, for example, submit comments using multiple versions of their names. These decisions can have a substantial impact on the analysis as misidentifying organizations may result in them being dropped or double-counted.  Scholars can ameliorate this issue by using fuzzy matching - a process that compares two strings of text and gives a distance between them. Choosing the optimal distance and thresholds is up to the researcher and usually an iterative process. Fuzzy matching, however, is not a panacea as the algorithm will show small distances between distinct entities (e.g., JP Morgan and Morgan Stanley), and it cannot separate multiple organizations with the same acronym (e.g., ABA).


<!--
^[This process is particularly difficult for comments submitted by a coalition.
Suppose a scholar is interested in the relationship between assets and commenting behavior.
If a coalition of the Bank of America and Wells Fargo comment on a rule, should the scholar use their combined assets as their independent variable of interest, their average assets, or not assign covariate data to this coalition?  There is no obvious answer to this question]
-->

# Who writes rules?

Political scientists have long used personal identity and social networks to understand political outcomes. New data sources have made it easier to learn about rulewriters' identities and networks. Some of the most important data sources in this regard are the U.S. Office of Personnel Management's (OPM) data on government employees [e.g., @BoltonAMP2018],^[Note that the dataset @BoltonAMP2018 used is not public.] Open Secrets' lobbying [e.g., @Baumgartner2009] and revolving door databases [e.g.,  @VidalAER2012;@BertrandAER2014], machine-readable lobbying disclosure act reports [e.g.,  @BoehmkeJPP2013;@YouJOP2017;@Dwidar2019SPSA], meeting logs [@LibgoberQJPS], and datasets of corporate board membership such as Boardex [e.g., @ShiveROF2016]. @CarriganPAR2019 illustrate the possibilities of these new data sources. They find that the number of job functions of the bureaucrats who write the rules is associated with both decreases in the time an agency takes to promulgate a rule and increases in the probability that the rule will be overturned in court. In another innovative study, @ChenJTP2015 link OPM data to Bonica's DIME database in order to classify the ideology of agency staff over time and apply these measures to study unionization. These early efforts suggest that opportunities to study who writes the rules and how it matters are prevalent. We expect such analyses to be a growth area for years to come.


<!-- Lobbying also increases the time it takes for agencies to discover corporate malfeasance [@YuJFQA2011].

-->

## Obtaining and working with personnel and lobbying data

Scholars can obtain data on agency personnel from the BuzzFeed personnel data release, which contains information such as employee salaries, job titles, and demographic data from 1973 through 2016.^[Available at https://archive.org/details/opm-federal-employment-data/page/n1. Updated personnel files are available through 2018 from the OPM itself here: https://www.fedscope.opm.gov/datadefn/index.asp]  These are the most comprehensive personnel records publicly available, but they have significant limitations.
First, not all agencies and occupations are a part of this release.^[The list includes at least 16 agencies and, within the covered agencies, law enforcement officers, nuclear engineers, and certain investigators @Singer-Vine2017.]  Second, some of the employees in these data do not have unique identification numbers. This means that common names such as 'John Smith' match multiple employees; the Veterans Health Administration employed 24 John Smiths in 2014, five with the same middle initial.

To obtain machine-readable data on the identities of domestic lobbyists, scholars use two databases from Open Secrets -- a nonprofit that tracks spending on politics -- on administrative and Congressional lobbying.
Downloading the lobbying data is straightforward; it only requires an account to access the 'bulk data' page.
The lobbying data comes from the required disclosures under the Lobbying Disclosure Act of 1995 (LDA).
Note that the reporting threshold varies by type of firm (in-house have a higher minimum reporting threshold than lobbying firms) and over time.^[See https://lobbyingdisclosure.house.gov/ldaguidance.pdf for details.] The lobbying data covers 1999 through 2018 and is broken up into seven machine-readable tables.^[These data can be downloaded here: https://www.senate.gov/legislative/Public_Disclosure/LDA_reports.htm]  However, these data do not contain exact monetary amounts, and some lobbyists do not disclose required contacts.^[See https://www.gao.gov/assets/700/698103.pdf]  Open Secret's database is easier to use than the raw data.
Open Secrets also has a database on revolving door employees that shows the career paths or federal government workers that went to the private sector and are employed in some capacity where their work depends on interacting with the federal government.^[See https://www.opensecrets.org/revolving/methodology.php]  Unlike their lobbying database, this one must be scraped as there is no option to download it using bulk data.<!--^[Note that they promote academic work using their database, so they do not actively discourage scraping their database.]    -->

Lobbyists advocating for foreign clients are required to disclose these contacts under the Foreign Agent Registration Act. Foreign agents often lobby bureaucratic agencies; Israel, for example, retained law firm Arnold & Porter for advice on registering securities with the SEC.^[See https://efile.fara.gov/docs/1750-Supplemental-Statement-20110729-13.pdf]  In addition to the participants, the reports also contain the nature of the contact and the specific officials contacted.
The data are astonishingly complete - the website contains all 6264 FARA registrants and their foreign contacts since  1942.^[https://efile.fara.gov/ords/f?p=1381:1:13132679194789:::::]  Unlike other data discussed in this essay, the FARA data is *not* all in machine-readable form.^[Only the metadata (e.g., names) are machine-readable.] Consequently, scholars using the data only focus on a subset such as @You2019 who focuses on lobbying activities by Colombia, Panama, and South Korea.
We caution users that the same participant can be listed under multiple names.

Corporate executives, lawyers, and lobbyists often have contacts with agency officials, called *ex parte* communications [@Lubbers2012], to discuss rules outside of the public comment process.
While record-keeping practices for these meetings vary considerably, they are often publicly available on agency websites^[Note that available does not mean conspicuous, as these logs are often hard to find by directly navigating the agency homepage.] Because meetings data are held in different places on each agency's website, obtaining these data requires writing a web scraper for each agency.
As there are no uniform standards for reporting meeting data, these data differ substantially from agency to agency in both content and organization.
The Federal Reserve, for example, groups meetings by subject but not by rule,^[https://www.federalreserve.gov/regreform/communications-with-public.htm] the SEC groups meetings by rule, and the CFTC and FCC has all their meetings in one place.^[See https://www.cftc.gov/LawRegulation/DoddFrankAct/ExternalMeetings?page=6 and https://www.fcc.gov/proceedings-actions/ex-parte/archive-of-filings] Meeting participants are typically individually identified, but some agencies only report the names of the organization represented, and others report meeting topics but not participants. No study evaluates the comprehensiveness of these record disclosures. We suspect that missingness varies by agency.

# What do the rules say?

The literature on rulemaking in political science and public administration has focused on whether public participation influences the content of rules [e.g., @BallaAPSR1998;@CuellarALR2005;@YackeeJOP2006;@NaughtonJPAM2009;@Wagner_ALR_2011;@HaederAPSR2015;@HaederJPART2018;@GordonRashinJOP]. Crucial to such commenter influence studies is knowing what the rules say and how these texts have changed from proposal to finalization. Especially useful to scholars is the rule's preamble, which describes in detail what the agency has tried to accomplish and how it has engaged with stakeholders. The legally operative text is often less useful to social scientists since interpreting legal text often requires substantial domain expertise. However, where interpretation is straightforward, such as rate-setting, scholars can capture variation in legally operative texts [@BallaAPSR1998;  @GordonRashinJOP].

<!--
^[However, see @GoldenJPART1998 and @ElliottDLJ1992 for evidence that rules do not change substantially.]
-->

## Obtaining and Working with Data on Rule Text

The federal government publishes all rules in the *Federal Register*, accessible via FederalRegister.gov.
This database is comprehensive and has machine-readable records of all regulations published after 1994 and optical versions form 1939-1993 (https://www.govinfo.gov/app/collection/fr/). The *Federal Register* API does not have any rate limits.
The Federal Register office assigns every regulatory action a document number. In our experience, this `FR Doc Number' is the only truly unique and consistently maintained identifier for proposed and final rules.

While the raw text of rules is relatively straightforward to obtain, there are several thorny theoretical and methodological issues that scholars must overcome.^[We note that raw texts of rules are not available from regulations.gov or the independent agencies themselves.
As these rules are often in PDF form, converting them to .txt for analysis introduces errors into the text.]  First, the standard path from notice of proposed rulemaking (NPRM) to public comments to a final rule is not always straightforward.
Some rules are withdrawn before a final rule.
Other agencies issue interim final rules subject to comments. For studies that seek to compare the proposed and final rules, the most problematic rules are the ones where one proposed rule gets broken up into a few smaller final rules or the reverse, where small rules become bundled into one final rule. These rules pose challenges for inference as the processes that lead to amalgamation or separation are not well understood.
Second, scholars must decide whether, and to what extent, they should preprocess the rulemaking data before feeding the data to a text analysis algorithm.


<!--
^[See, for example, the prudential standards rule issued by the Federal Reserve to improve financial stability.]
-->

# Obtaining and working with rule metadata

Rulemaking metadata, such as the time rules are released, allow scholars to answer questions about factors that affect the rulemaking environment.
Scholars use this data to study questions about regulatory delay [e.g., @ACSPSQ2013;@ThrowerPSQ2018;@PotterJOP2017;@CarriganPAR2019;@CarpenterAJPS2011], agenda-setting[e.g., @CoglianeseALR2016], and the financial impact of rules [e.g., @Libgober2018]. These scholars relied on data from the Office of Information and Regulatory Affairs (OIRA) [@ACSPSQ2013;@CarriganPAR2019], the Unified Agenda [@CoglianeseALR2016;@PotterJOP2017], the *Federal Register* [@ThrowerPSQ2018], press releases [@Libgober2018], and FDA's drug approval and postmarket experience [@CarpenterAJPS2011].


<!--
Workman's data isn't public, so I'm going to ax this graf
Scholars have also exploited the Unified Agenda to study agenda setting.
Notably, @Workman2015 coded approximately 220,000 regulations by issue area; he exploits this novel dataset to advance a theory of the "dual dynamics" of the administrative state where agenda-setting over the rulemaking process is a function of information generated by bureaucrats and Congressional "tuning" of the information.
@WestJPART2012 assess the influences on agency discretion using a survey of bureaucratic "contact officers" and the Unified Agenda; they find that most agency decisions to issue rules "are made in the context of ongoing program implementation."
-->

Obtaining data from the Unified Agenda is relatively straightforward as all of these documents since 1995 are available online in machine-readable form.^[The Unified Agenda from 1983 through 1994 is available in the *Federal Register*.]
The Unified Agenda contains many of the proposed regulations an agency plans to issue in the near future, making it an extremely useful data source for studying questions about agenda setting and timing [e.g., @Potter2019].
There are, however, significant limitations to these data.
First, agencies report their early-stage rulemaking to the Unified Agenda strategically [@NouSCLR2016].
Second, agencies do not list all 'failed' rules that did not become final rules in the Unified Agenda [@YackeeGWLR2012].
Third, @CoglianeseALR2016 note that the Unified Agenda misses much of the regulatory agency's work, including enforcement actions, adjudicatory actions, and decisions not to act.

As addressed elsewhere in this issue [NOTE: reference to OMB article here], many rules, especially rules that are controversial or deemed economically significant, are reviewed by OMB's Office of Information and Regulatory Affairs (OIRA). Obtaining OIRA data is relatively straightforward, as all of these documents since 1981 are available online in machine-readable form. These data include the date on which OIRA received the rule from the agency, whether the review was expedited, and whether the rule is determined to by 'economically significant' or affect 'federalism'.

In addition to disclosures mandated by law, agencies often issue press releases for agency actions.
Much like the meetings data discussed above, policies regarding the storage and dissemination of press releases differ from agency to agency.
The Federal Reserve, for example, lists all press releases since 1996 on their website, while the SEC only has them from 2012.^[See https://www.federalreserve.gov/newsevents/pressreleases.htm and https://www.sec.gov/news/pressreleases]  When working with press releases, scholars often need data on the exact time documents were made available to the general public [see, e.g., @LibgoberJOP].
Press release metadata, such as the exact time a press release becomes public, can often be extracted from Really Simple Syndication (RSS) feeds.

Each data source has different types of missingness and different limitations regarding the information it contains about each rule. For example, data from the Office of Information and Regulatory Affairs (OIRA) only contains rules reviewed by OMB. The Unified Agenda (UA) covers a broader scope of policies, but due to strategic reporting and frequent reporting errors, desired cases may be missing. For published rules, these missing cases may be found in a more reliable source like the *Federal Register*, which is more reliable than the UA, OIRA reports, or regulations.gov, and thus the best source for rule texts. However, the Federal Register contains the least amount of rule metadata and only published drafts and rules. This means that some rulemaking projects that were never finalized may be missing from all datasets. More importantly, the diversity of these data sources means that, for a given query, one source will often include cases that a second does not, while the second source includes variables that first does not.
<!--
^[We have used Feedly, but any RSS reader should have accessible timing metadata.]
-->  

# Discussion: Assembling Complete Databases  

The United States government releases troves of data on rulemaking but in forms that require substantial effort from scholars to be useful for research. Scholars working on bureaucratic politics face two primary data challenges going forward: assembling complete, machine-readable datasets of agency rulemaking activity and linking observations across datasets.
We see four fruitful data projects which could increase researcher efficiency by preventing duplicated efforts to download and clean data.
First, a complete database is needed to link commenting activity throughout the Federal government. Researchers should be able to download these data in bulk, including comment text and metadata.
Second, a comprehensive database could link revolving door rulemakers from the OPM personnel records, LDA disclosure forms, and FARA data.
This database would be significantly improved by the addition of data from LinkedIn, but those data are not currently public.
Third, creating a database of all meeting activities throughout the federal government.
Finally, unique identifiers for each commenter would allow researchers to link commenting behavior to organizations across datasets.
These projects, once completed, will allow scholars to pursue novel research on political participation, influence, and public management.

# Conflicts of Interest
On behalf of all authors, the corresponding author states that there is no conflict of interest.
