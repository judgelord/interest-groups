---
title: Data and Methods for Analyzing Special Interest Influence in Rulemaking

author:
- name: Daniel Carpenter
  affiliation: Harvard University
#  footnote: 1737 Cambridge Street, Room 405, Cambridge, Massachusetts 02138
- name: Devin Judge-Lord
  affiliation: University of Wisconsin
#  footnote: 1059 Bascom Mall, 110 North Hall, Madison, WI 53706
- name: Brian Libgober
  affiliation: Yale University
#  footnote: 115 Prospect Street, Room 113, New Haven, CT 06520
- name: Steven Rashin
  affiliation: New York University
  footnote: "Corresponding Author: Steven.Rashin@NYU.edu, 19 West 4th St, 2nd Floor, New York, NY 10012"

abstract: The United States government creates astonishingly complete records of policy creation in executive agencies. In this article, we describe the major kinds of data that have proven useful to scholars studying interest group behavior and influence in bureaucratic politics, how to obtain them, and challenges that we as users have encountered in working with these data. We discuss established databases such as regulations.gov, which contains comments on draft agency rules, and newer sources of data, such as ex-parte meeting logs, which describe the interest groups and individual lobbyists that bureaucrats are meeting face-to-face about proposed policies. One challenge is that much of these data are not machine-readable. We argue that scholars should invest in several projects to make these datasets machine-readable and to link them to each other as well as to other databases.

keywords: Interest groups, rulemaking, lobbying, bureaucratic politics, data sources

output:
   pdf_document:
      template: article-template.tex
      keep_tex: true
      latex_engine: pdflatex
      citation_package: natbib
      fig_caption: true
spacing: doublespacing
biblio-style: agsm.bst
bibliography: dataig.bib
endnotes: true # CHANGE THIS TO true BEFORE SUBMISSION
frontpage: true

---

\noindent Submission ID: IGAD-D-19-00044

<!--
%Please remember that manuscripts should be limited to 3,000 - 5,000 words, including all tables, notes, and bibliographies.
If you plan to provide an online appendix, you may include that appendix with your initial submission, omitting it when you submit the final manuscript for production. That online appendix would not count toward the word limit.

Citations
All submissions should follow the Harvard style of in-text parenthetical citations.

Notes
Explanatory notes should be kept to an absolute minimum. Final versions of articles must use endnotes, not footnotes.

Style
The original style goals are clarity and consistency. Authors may use either UK or US spelling, but they must do so consistently. For punctuation, please follow the standards for UK English. This entails:

- using single inverted commas (rather than double quotation marks) for quotes
- placing all inverted commas within other punctuation
- for the grammatical dash, using an en dash with a single space before and after the en dash (rather than an em dash without spaces)

R&R RESPONSE:
https://docs.google.com/document/d/1qxUJxtnuc4kl1-oEB7_ksZKmAxPtD9OVqogOmIFN8gs/edit?usp=sharing
-->

# Introduction

If the U.S. federal government is unquestionably good at one thing, it is pushing out paper.
In theory, governmental records relevant to the creation of policy via agency rulemaking have long been available to researchers. Given its volume and quasi-legislative nature, rulemaking is generally required to be more transparent than more individualized decision-making such as adjudication, enforcement, awarding grants, or making contracts. In practice, however, obtaining information on what is happening during rulemaking has been costly and challenging. In the 1990s and early 2000s, leading research was limited to data on just a few rules [e.g., @GoldenJPART1998] or surveys [e.g., @FurlongJPART2004].
Since 1994, when the government first released the *Federal Register* online, data on notice-and-comment rulemaking have become increasingly detailed and now include data on draft policies, the activities of policymakers, and interest group advocacy [see @Yackee2019 for a recent review].

In this article, we describe data sources that have proven useful to scholars studying interest group lobbying of federal agencies, how to obtain them, and also challenges in working with these data. Some of these data sources we discuss are machine-readable records of all agency rules published in the *Federal Register*, comments posted on regulations.gov, metadata about rules contained in the Unified Agenda, and Office of Management and Budget (OMB) reports. We also describe sources that have become more available in recent years, such as ex-parte meeting logs and individually-identified personnel records of nearly all federal employees since 1973. For background theory on lobbying and interest group influence over agency policymaking, see @Carpenter2013.

These data sources do not exhaust the kinds of records relevant to researchers. They reflect what is available at present, but new data sources emerge constantly. Thus, we also highlight datasets that may soon become more accessible or available, perhaps after enterprising researchers submit the necessary Freedom Of Information Act (FOIA) requests. Examples include agency press releases [see, e.g., @LibgoberQJPS;@Libgober2018] and the Foreign Agents Registration Act (FARA) reports [@ShepherdAPSR2019].^[We link to more relevant data sources on our GitHub page: <https://github.com/libgober/regdata/blob/master/README.md.>]  To orient potential students of agency policymaking to available sources of data, we identify four units of analysis: (1) participants, (2) policymakers, (3) policy texts, and (4) metadata (e.g., policy timing). We describe where to find data on each in roughly the order that they appear in the process of developing a rule.

## Background: Rulemaking and the Administrative Procedures Act

For decades, the volume of legal requirements emerging from executive agencies has dwarfed the lawmaking activity of Congress.
Each year, agencies publish three thousand or more regulations. In doing so, agencies must generally follow a process prescribed by the Administrative Procedures Act (APA), 5 U.S.C. ยง 553(c).
For each rule, agencies create a collection of documents at each stage of their policymaking process.
Section 553 requires agencies to issue a Notice of Proposed Rulemaking (NPRM) in the *Federal Register* to notify potentially affected parties that the regulatory environment or program administration may change.
Following the NPRM, agencies post these proposed rules and comments from interested stakeholders and individuals on their websites or regulations.gov.
Agencies are required to consider public comments but are not required to alter rules based on them.
These documents are organized in a 'docket' folder with a unique name and ID number. Once kept in literal folders in each agency's 'docket room', where visitors could make photocopies, these documents are now online.
For example, the Centers for Medicare & Medicaid Services rulemaking docket on health plans disclosing costs can be accessed at [regulations.gov/docket?D=CMS-2019-0163](regulations.gov/docket?D=CMS-2019-0163).
The fact that the APA requires agencies to assemble a comprehensive record of who sought to influence the policymaking process -- and that such records are relatively complete -- is unusual for federal policymaking. This makes agency policymaking particularly exciting for interest group scholars.

# Who participates in the rulemaking process?

Public attention in rulemaking is remarkably skewed. A few rules, such as the Federal Communication Commission's rules on net neutrality, receive millions of comments. In contrast, half of the proposed rules open for comment on regulations.gov received no comments at all [@LibgoberJOP]. Such dramatic variation in public participation challenges the idea of a "typical" rule or rulemaking participant. Participants include businesses, public interest groups, trade associations, unions, law firms, and academics [@CuellarALR2005;@YackeeJOP2006]. While businesses are the most consistent and influential participants in most rulemakings [@YackeeJOP2006;@LibgoberJOP], most comments come from public pressure campaigns targeting a relatively small number of rules; at least 39 million of the 48 million comments on proposed rules on regulations.gov were mobilized by just 100 advocacy organizations such as the Sierra Club [@judgelord2019SPSA].

<!--
Even the median rule designated as 'economically significant' -- rules projected to have an annual economic impact of over $100 million -- receives fewer than ten comments [@judgelord2019SPSA].
-->

Because interest groups seek to influence rules, scholars are interested in patterns of participation in rulemaking.
For example, @YouJOP2017 finds that half of all spending on lobbying legislation occurs *after* a bill becomes law.
Participation in this process can begin even before a proposed rule is issued.
@deFigureidoKimICC2004 show that meetings between agency officials and firms spike before an agency issues a policy.
@LibgoberQJPS finds that firms that meet with federal regulators before a rule is issued may receive abnormally high stock market returns upon its release. Leveraging high-frequency trading data, Libgober shows that in the minutes and hours following the publication of proposed rules at the Federal Reserve, the firms that met with the Board during rule-development significantly outperformed matched market competitors that did not obtain such early access. These findings are consistent with the analysis of qualitative researchers that commenters who participate early in the rulemaking process can shape the content of the rule [@NaughtonJPAM2009].

<!--
Interest groups and firms may even initiate a rulemaking process through a petition for rulemaking under 5 U.S.C. ยง 553(e).
-->

Scholars are also interested in how participation in rulemaking may affect a rule's ultimate fate when it is sent to the White House Office of Management and Budget (OMB) or in judicial review.
Interest groups can secure policy concessions by lobbying during OMB review [@HaederAPSR2015].  They also use the rulemaking record to build a case for litigation.
Yet, there are few large-N studies linking lobbying in rulemaking to litigation or court decisions.
@LibgoberRashin2018SPSA analyzed comments submitted to financial regulators and found that threats of litigation were rare, and even comments that seemed to "threaten" litigation rarely culminated in a judicial decision. Yet, most court decisions involving challenges to rules did follow comments that threatened litigation.
@judgelord2016MPSA found no relationship between the number of comments and the likelihood that the Supreme Court upheld or struck down an agency rule.

<!--
and, like @SchuckElliott1990, found that the court is more likely to strike down agency policies made through the notice and comment process.
-->

To analyze patterns of participation, scholars use data from a variety of sources.
Sources for data on participation are agency rulemaking dockets [@GoldenJPART1998;@YackeeJPART2006;@YoungBP2017;@BanBP2019], the *Federal Register* [@BallaAPSR1998;@WestPAR2004], and regulations.gov [@BallaPI2019;@GordonRashinJOP].
Though commenters are not generally required to disclose their names and affiliations, many do. The best current data sources for obtaining the names of the organizations that submit comments on federal regulations are regulations.gov and the websites of the independent agencies themselves.

<!--
Stuff removed for length
^[One guide to effective commenting notes that organized interests should tell the regulators exactly who they are and why the regulators should alter a rule based on their comment [@Stoll2011].]
^[Individual states also have rulemaking procedures; every state now has a version of the Administrative Procedures Act in place [see, e.g., @JensenPRQ2010]. Scholarly work has analyzed commenting and lobbying behavior in Minnesota [@BoehmkeJPP2013] and California [@JewellJPART2006] among other states.]

-->

## Obtaining and working with data on comment participants

Scholars wishing to obtain data on comments and commenters from executive agencies generally use the website \href{regulations.gov}{regulations.gov}.
Obtaining these data requires overcoming several technical and bureaucratic hurdles.
First, retrieving bulk data from regulations.gov requires the use of an Application Programming Interface (API).^[An API is a set of procedures that allow a user to access data from a website in a structured way.
Some websites, like regulations.gov, limit API usage by requiring users to get an API key, see <https://regulationsgov.github.io/developers/>.]
<!---
^[By emailing regulations@erulemakinghelpdesk.com with 'your name, email address, organization, and intended use of the API'.  
Approval of API keys takes several days. We caution prospective users of the site that regulations.gove may deactivate these keys without warning or explanation.]
-->
Second, getting an accurate count of comments is not straightforward as agencies have different policies regarding duplicated comments^[The difference between the number of reported comments and the number of comments on regulations.gov is often because some agencies group mass-comment campaigns into a single document. A small number of comments on regulations.gov are duplicates posted in error. To resolve a discrepancy, we recommend searching the agency's website or contacting the agency directly.] and confidential business information [@Lubbers2012].  

Third, downloading these data can be time-consuming.
As of February 29, 2020, Regulations.gov has 12,227,522 public submission documents representing over 70 million public comments, over 80 thousand rules, and nearly 1.5 million other documents. 
Regulations.gov is subject to a rate limit of 1,000 queries per hour, making it difficult for scholars seeking to analyze rules with tens of thousands of comments. 
This limit is particularly problematic for rules where the majority of comments are attachments, as downloading an individual document requires calling the API twice (once for the docket information, which includes the attachment URL(s), and then a second time to download the linked file).
<!---
problematic for scholars seeking to analyze the full text of comments, many of which are as attachments. Downloading an individual document requires calling the API twice, once for the docket information, which includes the attachment URL(s), and then a second time to download the linked file.
--->
Fourth, not all federal agencies post rulemaking documents to regulations.gov. 
For these agencies, scholars studying participation in rulemaking can often obtain data from the agency's website.^[For example, the Federal Energy Regulatory Commission has posted all comments they receive on their eLibrary website, but not all of these appear on regulations.gov.
Unlike regulations.gov, most agency sites do not have an API and thus require bespoke web scrapers.
We offer examples of scrapers for regulations.gov and several of these agencies on <https://github.com/libgober/regdata>.]

<!--
While these websites are not rate-limited or gated with API keys, we recommend caution with the rate at which scholars solicit information from the servers to avoid security protocols that trigger a temporary ban.]  

For example, on an EPA rule on clean water, regulations.gov notes that 1,123,388 million comments were submitted, but only 20,594 are available on the website itself. FDA is famous for this as well. Susan got them from FOIA.

Based on experience, we have found that attachments cannot be downloaded at a rate of 1,000 per hour as some documents will fail to download or download as files with 0 bytes.
Instead, we recommend a delay of several seconds in between each iteration.
This approach, however, vastly increases the time required to scrape large numbers of comments.
The OCC, for example, withholds confidential business information while HHS and IRS do not.^[See e.g., https://www.federalregister.gov/documents/2019/09/04/2019-18992/agency-information-collection-activities-information-collection-renewal-comment-request-joint and https://www.federalregister.gov/documents/2019/05/03/2019-09121/request-for-information-regarding-state-relief-and-empowerment-waivers.]

^[We recommend using rvest if coding in R and Scrapy in Python.
We have tutorials on how to scrape comments from regulations.gov and FERC.]
-->

\indent After obtaining comment data, scholars must choose how they want to preprocess the names of the organizations that comment.  This step is vital as organizations such as Goldman Sachs and the American Bar Association, for example, submit comments using multiple versions of their names. These decisions can have a substantial impact on the analysis as misidentifying organizations may result in them being dropped or double-counted.  Fuzzy matching, using an algorithm to identify the similarity (or "distance") between two strings of text, can help but is not a panacea. For example, the algorithm will show a small distance between Goldman Sachs and Goldman Sachs \& Co but can show similarly small distances between distinct entities and it cannot separate multiple organizations with the same acronym (e.g., the American Bankers' Association and American Bar Association both comment as the ABA). 


<!--
^[This process is particularly difficult for comments submitted by a coalition.
Suppose a scholar is interested in the relationship between assets and commenting behavior.
If a coalition of the Bank of America and Wells Fargo comment on a rule, should the scholar use their combined assets as their independent variable of interest, their average assets, or not assign covariate data to this coalition?  There is no obvious answer to this question]
-->

# Who writes rules?

Political scientists have long used personal identity and social networks to understand political outcomes. In the last decade, the emergence of new data sources made it easier to learn about rulewriters' identities and networks. Some of the most important data sources in this regard are the U.S. Office of Personnel Management's (OPM) data on government employees [e.g., @BoltonAMP2018],^[Note that the dataset @BoltonAMP2018 used is not public.] Open Secrets' lobbying [e.g., @Baumgartner2009] and revolving door databases [e.g.,  @VidalAER2012;@BertrandAER2014], machine-readable lobbying disclosure act reports [e.g.,  @BoehmkeJPP2013;@YouJOP2017;@Dwidar2019SPSA], meeting logs [@LibgoberQJPS], and datasets of corporate board membership such as Boardex [e.g., @ShiveROF2016]. @CarriganPAR2019 illustrate the possibilities of these new data sources. They find that the number of job functions of the bureaucrats who write the rules is associated with both decreases in the time an agency takes to promulgate a rule and increases in the probability that the rule will be struck down in court. In an innovative study of unionization, @ChenJTP2015 link OPM data to Adam Bonica's DIME ideology scores to classify the ideology of agency staff over time. These early efforts suggest that there are exciting opportunities to study who writes the rules and how it matters.


<!-- Lobbying also increases the time it takes for agencies to discover corporate malfeasance [@YuJFQA2011].

-->

## Obtaining and working with personnel and lobbying data

Scholars can obtain data on agency personnel from the BuzzFeed personnel data release, which contains information such as employee salaries, job titles, and demographic data from 1973 through 2016.^[Available at <https://archive.org/details/opm-federal-employment-data/page/n1>. Updated personnel files are available through 2018 from the OPM itself here: <https://www.fedscope.opm.gov/datadefn/index.asp>]  These are the most comprehensive personnel records publicly accessible, but they have limitations.
First, not all agencies and occupations are a part of this release.^[These data exclude at least 16 agencies and, within the covered agencies, law enforcement officers, nuclear engineers, and certain investigators @Singer-Vine2017.]  Second, some of the employees in these data do not have unique identification numbers, and common names such as 'John Smith' match multiple employees.  For example, the Veterans Health Administration employed 24 John Smiths in 2014, five with the same middle initial.

To obtain machine-readable data on the identities of domestic lobbyists, scholars use two databases from Open Secrets -- a nonprofit that tracks spending on politics -- on administrative and Congressional lobbying.
Downloading the lobbying data is straightforward; it only requires an account to access the 'bulk data' page.
The lobbying data comes from the required disclosures under the Lobbying Disclosure Act of 1995 (LDA).
Note that the reporting threshold varies by type of firm (in-house have a higher minimum reporting threshold than lobbying firms) and over time.^[See <https://lobbyingdisclosure.house.gov/ldaguidance.pdf> for details.] The lobbying data covers 1999 through 2018 and is broken up into seven machine-readable tables.^[These data are here: <https://www.opensecrets.org/bulk-data/download?f=Lobby.zip>] We prefer the Open Secrets data to the raw Senate data as the Open Secrets version contains more machine-readable information. 
<!--

-->
Lobbying data is subject to several limitations; the reports do not contain exact monetary amounts and some lobbyists do not disclose required contacts.^[See <https://www.gao.gov/assets/700/698103.pdf>] 
Open Secrets also has a database on revolving door employees that shows the career paths or federal government workers that went to the private sector work that depends on interacting with the federal government.^[See <https://www.opensecrets.org/revolving/methodology.php>.  Unlike their lobbying database, this dataset has no bulk data option and must be scraped.]
<!--^[Note that they promote academic work using their database, so they do not discourage scraping their database.]    -->

Lobbyists advocating for foreign clients are required to disclose these contacts under the Foreign Agent Registration Act. Foreign agents often lobby bureaucratic agencies; Israel, for example, retained law firm Arnold & Porter for advice on registering securities with the Securities and Exchange Commission.^[See <https://efile.fara.gov/docs/1750-Supplemental-Statement-20110729-13.pdf>]  In addition to the participants, the reports also contain the nature of the contact and the specific officials contacted.
These data are astonishingly complete -- the website contains all 6264 FARA registrants and their foreign contacts since 1942.^[See <https://efile.fara.gov/ords/f?p=1381:1:13132679194789:::::>] The FARA data is *not* all in machine-readable form.^[Only the metadata (e.g., names) are machine-readable.] Consequently, scholars using these data only focus on a subset such as @You2019 who focuses on lobbying activities by Colombia, Panama, and South Korea.
We caution users that the same participant may appear under multiple names.

Corporate executives, lawyers, and lobbyists often have contacts with agency officials, called *ex parte* communications [@Lubbers2012], to discuss rules outside of the public comment process.
While record-keeping practices for these meetings vary considerably, they are often publicly available on agency websites. Because meetings data are held in different places on each agency's website, obtaining these data requires writing a web scraper for each agency.
As there are no uniform standards for reporting meeting data, these data differ substantially from agency to agency in both content and organization. 
The Federal Reserve, for example, groups meetings by subject, not by rule,^[<https://www.federalreserve.gov/regreform/communications-with-public.htm>] the SEC groups meetings by rule, and the Commodity Futures Trading Commission (CFTC)
 and Federal Communication Commission (FCC) meeting records are not grouped at all.^[See <https://www.cftc.gov/LawRegulation/DoddFrankAct/ExternalMeetings?page=6> and <https://www.fcc.gov/proceedings-actions/ex-parte/archive-of-filings>]
Meeting participants are typically individually identified, but some agencies only report the names of the organization represented, and others report meeting topics but not participants. 
We suspect that missingness varies by agency, but are not aware of any study evaluating missingness in meeting record disclosures. 

# What do the rules say?

The literature on rulemaking in political science and public administration has focused on whether public participation influences the content of rules [e.g., @BallaAPSR1998;@CuellarALR2005;@YackeeJOP2006;@NaughtonJPAM2009;@Wagner_ALR_2011;@HaederAPSR2015;@HaederJPART2018;@GordonRashinJOP;@Rashin2019]. 
Knowing what the rules say and how these texts have changed from proposal to finalization is crucial studying commenter influence. 
Rule preambles, which describe what the agency has tried to accomplish and how it has engaged with stakeholders, are especially useful. The legally operative text is often less useful to social scientists because interpreting legal text usually requires substantial domain expertise. However, where interpretation is straightforward, such as rate-setting, scholars can capture variation in legally operative texts [@BallaAPSR1998;  @GordonRashinJOP].

<!--
^[However, see @GoldenJPART1998 and @ElliottDLJ1992 for evidence that rules do not change substantially.]
-->

## Obtaining and Working with Data on Rule Text

The federal government publishes all rules in the *Federal Register*, accessible via FederalRegister.gov.
This database is comprehensive and has machine-readable records of all regulations published after 1994 and optical versions form 1939-1993.^[See <https://www.govinfo.gov/app/collection/fr/>] The *Federal Register* API does not have any rate limits.
The Federal Register office assigns every regulatory action a document number. In our experience, this `FR Doc Number' is the only truly unique and consistently maintained identifier for proposed and final rules.

While the raw text of rules is relatively straightforward to obtain, there are several thorny theoretical and methodological issues that scholars must overcome.^[However, because rules are usually published as PDFs, converting them to raw text for analysis introduces errors.]  First, the standard path from NPRM to public comments to final rule is not always straightforward.
Some rules are withdrawn before a final rule.
Other agencies issue interim final rules subject change. For studies that seek to compare the proposed and final rules, the most problematic rules are the ones where one proposed rule gets broken up into a few smaller final rules or the reverse, where short rules become bundled into one final rule.
These rules pose challenges for inference as the processes that lead to amalgamation or separation are not well understood.
Second, scholars must determine how to preprocess the rulemaking data before feeding these data to a text analysis algorithm.


<!--
^[See, for example, the prudential standards rule issued by the Federal Reserve to improve financial stability.]
-->

# Obtaining and working with rule metadata

Rulemaking metadata, such as the time rules are released, allow scholars to answer questions about factors that affect the rulemaking environment.
Scholars use this data to study questions about regulatory delay [e.g., @ACSPSQ2013;@ThrowerPSQ2018;@PotterJOP2017;@CarriganPAR2019;@CarpenterAJPS2011], agenda-setting[e.g., @CoglianeseALR2016], and the financial impact of rules [e.g., @LibgoberQJPS]. These scholars relied on data from the Office of Information and Regulatory Affairs (OIRA) [@ACSPSQ2013;@CarriganPAR2019], the Unified Agenda [@CoglianeseALR2016;@PotterJOP2017], the *Federal Register* [@ThrowerPSQ2018], press releases [@Libgober2018], and the Food and Drug Administration's drug approval and postmarket experience database [@CarpenterAJPS2011].


<!--
Workman's data isn't public, so I'm going to ax this graf
Scholars have also exploited the Unified Agenda to study agenda setting.
Notably, @Workman2015 coded approximately 220,000 regulations by issue area; he exploits this novel dataset to advance a theory of the "dual dynamics" of the administrative state where agenda-setting over the rulemaking process is a function of information generated by bureaucrats and Congressional "tuning" of the information.
@WestJPART2012 assess the influences on agency discretion using a survey of bureaucratic "contact officers" and the Unified Agenda; they find that most agency decisions to issue rules "are made in the context of ongoing program implementation."
-->

Obtaining data from the Unified Agenda is relatively straightforward as all of these documents since 1995 are available online in machine-readable form.^[The Unified Agenda from 1983 through 1994 is available in the *Federal Register*.]
The Unified Agenda contains many of the proposed regulations that agencies plan to issue in the near future, making it an extremely useful data source for studying questions about agenda setting and timing [e.g., @Potter2019].
There are, however, significant limitations to these data.
First, agencies report early-stage rulemaking to the Unified Agenda strategically [@NouSCLR2016].
Second, agencies do not list all 'failed' rules that did not become final rules in the Unified Agenda [@YackeeGWLR2012].
Third, @CoglianeseALR2016 note that the Unified Agenda misses much of the regulatory agency's work, including enforcement actions, adjudicatory actions, and decisions not to act.

As addressed elsewhere in this issue [NOTE: reference to OMB article here], many rules, especially rules that are controversial or deemed economically significant, are reviewed by OIRA (a subagency of OMB). Obtaining OIRA data is relatively straightforward, as all of these documents since 1981 are available online in machine-readable form. These data include the date on which OIRA received the draft rule from the agency, whether the review was expedited, and whether the rule is determined to be 'economically significant' or affect 'federalism'.

In addition to disclosures mandated by law, agencies often issue press releases for agency actions.
Much like the meetings data discussed above, policies regarding the storage and dissemination of press releases differ from agency to agency.
The Federal Reserve's website, for example, lists all press releases since 1996, while the SEC only has them since 2012.^[See <https://www.federalreserve.gov/newsevents/pressreleases.htm> and <https://www.sec.gov/news/pressreleases>]  When working with press releases, scholars often need data on the exact time documents were made available to the general public [see, e.g., @LibgoberJOP].
Press release metadata, such as the exact time a press release becomes public, can often be extracted from Really Simple Syndication (RSS) feeds.

Each data source has different types of missingness and different limitations regarding the information it contains about each rule. For example, data from OIRA only contains rules reviewed by OMB. The Unified Agenda covers a broader scope of policies, but due to strategic reporting and frequent reporting errors, desired cases may be missing. For published draft and final rules, these missing cases may be found in a more reliable source like the *Federal Register*, OIRA reports, or regulations.gov. The *Federal Register* is the most reliable source for rule texts but contains the least amount of rule metadata and only includes *published* draft and final rules. Some rulemaking projects that did not reach the published draft (NPRM) stage may be missing from all datasets. More importantly, the diversity of these data sources means that, for a given query, one source will often include cases that a second does not, while the second source includes variables that first does not.
<!--
^[We have used Feedly, but any RSS reader should have accessible timing metadata.]
-->  

# Discussion: Assembling Complete Databases  

The United States government releases troves of data on rulemaking. Yet, these data require substantial effort from scholars to be useful for research. Scholars working on bureaucratic politics face two primary data tasks: (1) assembling complete, machine-readable datasets of agency rulemaking activity and (2) linking observations across datasets.
In the near term, we see four major projects to make data more accessible, prevent duplicated efforts to download and clean data, and thus increase the efficiency of research efforts.
First, a complete database is needed to link commenting activity throughout the Federal government. Researchers should be able to query and download these data in bulk, including comment text and metadata.
Second, a comprehensive database could link observations of the revolving door for rulewriters and participants across OPM personnel records, LDA disclosure forms, and FARA data.
This database could be augmented with data from networking sites like LinkedIn.
Third, a database could link all meeting activities throughout the federal government.
Finally, unique identifiers for each commenter and organization would allow researchers to link commenting behavior across datasets and to other information about these organizations.
These projects would allow scholars to pursue novel research on political participation, influence, and public management.

# Conflicts of Interest
On behalf of all authors, the corresponding author states that there is no conflict of interest.
